{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autorec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configurations\n",
    "First, imports should be loaded with the right configurations set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from autorecsys.auto_search import Search\n",
    "from autorecsys.pipeline import Input, LatentFactorMapper, DenseFeatureMapper, SparseFeatureMapper, \\\n",
    "                        ElementwiseInteraction, FMInteraction, MLPInteraction, ConcatenateInteraction, \\\n",
    "                        CrossNetInteraction, SelfAttentionInteraction, HyperInteraction, \\\n",
    "                        PointWiseOptimizer\n",
    "from autorecsys.pipeline.preprocessor import CriteoPreprocessor, AvazuPreprocessor\n",
    "from autorecsys.recommender import CTRRecommender\n",
    "\n",
    "# logging setting\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to build the interactors\n",
    "To perform benchmarking on different interactors, we will define functions to build the different interactor types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dlrm(emb_dict):\n",
    "    if 'user' in emb_dict or 'item' in emb_dict:\n",
    "        emb_list = [emb for _, emb in emb_dict.items()]\n",
    "        output = MLPInteraction(num_layers=2)(emb_list)\n",
    "    else:\n",
    "        sparse_feat_mlp_output = [MLPInteraction()( [emb_dict['sparse']] )] if 'sparse' in emb_dict else []\n",
    "        dense_feat_mlp_output = [MLPInteraction()( [emb_dict['dense']] )] if 'dense' in emb_dict else []\n",
    "        output = MLPInteraction(num_layers=2)(sparse_feat_mlp_output + dense_feat_mlp_output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_deepfm(emb_dict):\n",
    "    if 'user' in emb_dict or 'item' in emb_dict:\n",
    "        emb_list = [emb for _, emb in emb_dict.items()]\n",
    "        fm_output = [FMInteraction()(emb_list)]\n",
    "        bottom_mlp_output = [MLPInteraction(num_layers=2)(emb_list)]\n",
    "        output = MLPInteraction(num_layers=2)(fm_output + bottom_mlp_output)\n",
    "    else:\n",
    "        fm_output = [FMInteraction()( [emb_dict['sparse']] )] if 'sparse' in emb_dict else []\n",
    "        bottom_mlp_output = [MLPInteraction()( [emb_dict['dense']] )] if 'dense' in emb_dict else []\n",
    "        output = MLPInteraction(num_layers=2)(fm_output + bottom_mlp_output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_crossnet(emb_dict):\n",
    "    if 'user' in emb_dict or 'item' in emb_dict:\n",
    "        emb_list = [emb for _, emb in emb_dict.items()]\n",
    "        fm_output = [CrossNetInteraction()(emb_list)]\n",
    "        bottom_mlp_output = [MLPInteraction(num_layers=2)(emb_list)]\n",
    "        output = MLPInteraction(num_layers=2)(fm_output + bottom_mlp_output)\n",
    "    else:\n",
    "        fm_output = [CrossNetInteraction()( [emb_dict['sparse']] )] if 'sparse' in emb_dict else []\n",
    "        bottom_mlp_output = [MLPInteraction()( [emb_dict['dense']] )] if 'dense' in emb_dict else []\n",
    "        output = MLPInteraction(num_layers=2)(fm_output + bottom_mlp_output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_autoint(emb_dict):\n",
    "    if 'user' in emb_dict or 'item' in emb_dict:\n",
    "        emb_list = [emb for _, emb in emb_dict.items()]\n",
    "        fm_output = [SelfAttentionInteraction()(emb_list)]\n",
    "        bottom_mlp_output = [MLPInteraction(num_layers=2)(emb_list)]\n",
    "        output = MLPInteraction(num_layers=2)(fm_output + bottom_mlp_output)\n",
    "    else:\n",
    "        fm_output = [SelfAttentionInteraction()( [emb_dict['sparse']] )] if 'sparse' in emb_dict else []\n",
    "        bottom_mlp_output = [MLPInteraction()( [emb_dict['dense']] )] if 'dense' in emb_dict else []\n",
    "        output = MLPInteraction(num_layers=2)(fm_output + bottom_mlp_output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_neumf(emb_dict):\n",
    "    emb_list = [emb for _, emb in emb_dict.items()]\n",
    "    innerproduct_output = [ElementwiseInteraction(elementwise_type=\"innerporduct\")(emb_list)]\n",
    "    mlp_output = [MLPInteraction(num_layers=2)(emb_list)]\n",
    "    output = innerproduct_output + mlp_output\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_autorec(emb_dict):\n",
    "    if 'user' in emb_dict or 'item' in emb_dict:\n",
    "        emb_list = [emb for _, emb in emb_dict.items()]\n",
    "        output = HyperInteraction()(emb_list)\n",
    "    else:\n",
    "        sparse_feat_bottom_output = [HyperInteraction(meta_interator_num=2)([sparse_feat_emb])] if 'sparse' in emb_dict else []\n",
    "        dense_feat_bottom_output = [HyperInteraction(meta_interator_num=2)([dense_feat_emb])] if 'dense' in emb_dict else []\n",
    "        top_mlp_output = HyperInteraction(meta_interator_num=2)(sparse_feat_bottom_output + dense_feat_bottom_output)\n",
    "        output = HyperInteraction(meta_interator_num=2)([top_mlp_output])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process input\n",
    "Parse the arguments provided from program input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # parse args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-model', type=str, help='input a model name', default='dlrm')\n",
    "    parser.add_argument('-data', type=str, help='dataset name', default=\"avazu\")\n",
    "    parser.add_argument('-data_path', type=str, help='dataset path', default='./example_datasets/avazu/train-10k')\n",
    "    parser.add_argument('-sep', type=str, help='dataset sep')\n",
    "    parser.add_argument('-search', type=str, help='input a search method name', default='random')\n",
    "    parser.add_argument('-batch_size', type=int, help='batch size', default=256)\n",
    "    parser.add_argument('-trials', type=int, help='try number', default=2)\n",
    "    parser.add_argument('-gpu_index', type=int, help='the index of gpu to use', default=0)\n",
    "    args = parser.parse_args()\n",
    "    print(\"args:\", args)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_index)\n",
    "\n",
    "    if args.sep == None:\n",
    "        args.sep = '::'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "We then load the preprocessed Criteo or Avazu data depending on the input arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if args.data == \"avazu\":\n",
    "        # Step 1: Preprocess data\n",
    "        avazu = AvazuPreprocessor(csv_path=args.data_path, validate_percentage=0.1, test_percentage=0.1)\n",
    "        train_X, train_y, val_X, val_y, test_X, test_y = avazu.preprocess()\n",
    "        train_X_categorical = avazu.get_x_categorical(train_X)\n",
    "        val_X_categorical = avazu.get_x_categorical(val_X)\n",
    "        test_X_categorical = avazu.get_x_categorical(test_X)\n",
    "        categorical_count = avazu.get_categorical_count()\n",
    "        hash_size = avazu.get_hash_size()\n",
    "\n",
    "        # Step 2: Build the recommender, which provides search space\n",
    "        # Step 2.1: Setup mappers to handle inputs\n",
    "        # dense_input_node = None\n",
    "        sparse_input_node = Input(shape=[categorical_count])\n",
    "        input = [sparse_input_node]\n",
    "\n",
    "        # dense_feat_emb = None\n",
    "        sparse_feat_emb = SparseFeatureMapper(\n",
    "            num_of_fields=categorical_count,\n",
    "            hash_size=hash_size,\n",
    "            embedding_dim=64)(sparse_input_node)\n",
    "\n",
    "        emb_dict = {'sparse': sparse_feat_emb}\n",
    "\n",
    "    if args.data == \"criteo\":\n",
    "        # Step 1: Preprocess data\n",
    "        criteo = CriteoPreprocessor(csv_path=args.data_path, validate_percentage=0.1, test_percentage=0.1)\n",
    "        train_X, train_y, val_X, val_y, test_X, test_y = criteo.preprocess()\n",
    "        train_X_numerical, train_X_categorical = criteo.get_x_numerical(train_X), criteo.get_x_categorical(train_X)\n",
    "        val_X_numerical, val_X_categorical = criteo.get_x_numerical(val_X), criteo.get_x_categorical(val_X)\n",
    "        test_X_numerical, test_X_categorical = criteo.get_x_numerical(test_X), criteo.get_x_categorical(test_X)\n",
    "        numerical_count = criteo.get_numerical_count()\n",
    "        categorical_count = criteo.get_categorical_count()\n",
    "        hash_size = criteo.get_hash_size()\n",
    "        \n",
    "        # Step 2: Build the recommender, which provides search space\n",
    "        # Step 2.1: Setup mappers to handle inputs\n",
    "        dense_input_node = Input(shape=[numerical_count])\n",
    "        sparse_input_node = Input(shape=[categorical_count])\n",
    "        input = [dense_input_node, sparse_input_node]\n",
    "\n",
    "        dense_feat_emb = DenseFeatureMapper(\n",
    "            num_of_fields=numerical_count,\n",
    "            embedding_dim=64)(dense_input_node)\n",
    "\n",
    "        sparse_feat_emb = SparseFeatureMapper(\n",
    "            num_of_fields=categorical_count,\n",
    "            hash_size=hash_size,\n",
    "            embedding_dim=64)(sparse_input_node)\n",
    "\n",
    "        emb_dict = {'dense': dense_feat_emb, 'sparse': sparse_feat_emb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the recommender\n",
    "The next step is to build the recommender by setting up the mapper, interactor, and optimizer. The recommender provides the search space for the searcher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 2.2: Setup interactors to handle models\n",
    "    if args.model == 'dlrm':\n",
    "        output = build_dlrm(emb_dict)\n",
    "    if args.model == 'deepfm':\n",
    "        output = build_deepfm(emb_dict)\n",
    "    if args.model == 'crossnet':\n",
    "        output = build_neumf(emb_dict)\n",
    "    if args.model == 'autoint':\n",
    "        output = build_autorec(emb_dict)\n",
    "    if args.model == 'autorec':\n",
    "        output = build_autorec(emb_dict)\n",
    "\n",
    "    # Step 2.3: Setup optimizer to handle the target task\n",
    "    output = PointWiseOptimizer()(output)\n",
    "    model = CTRRecommender(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the searcher\n",
    "The search function of AutoRec will now use the search algorithm to find the models with the best accuracies and show the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 3: Build the searcher, which provides search algorithm\n",
    "    searcher = Search(model=model,\n",
    "                      tuner=args.search,\n",
    "                      tuner_params={'max_trials': args.trials, 'overwrite': True}\n",
    "                      )\n",
    "\n",
    "    # Step 4: Use the searcher to search the recommender\n",
    "    start_time = time.time()\n",
    "    searcher.search(x=train_X,\n",
    "                    y=train_y,\n",
    "                    x_val=val_X,\n",
    "                    y_val=val_y,\n",
    "                    objective='val_BinaryCrossentropy',\n",
    "                    batch_size=args.batch_size,\n",
    "                    epochs=1,\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)]\n",
    "                    )\n",
    "    end_time = time.time()\n",
    "    print(\"running time:\", end_time - start_time)\n",
    "    print(\"args\", args)\n",
    "    logger.info('Validation Accuracy (logloss): {}'.format(searcher.evaluate(x=val_X,\n",
    "                                                                             y_true=val_y)))\n",
    "\n",
    "    # Step 5: Evaluate the searched model\n",
    "    logger.info('Test Accuracy (logloss): {}'.format(searcher.evaluate(x=test_X,\n",
    "                                                                       y_true=test_y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
